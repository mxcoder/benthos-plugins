logger:
  level: INFO

input:
  read_until:
    check: count("messages") >= 5
    input:
      kafka:
        addresses: [${KAFKA_ADDRESS}]
        topics: ["${KAFKA_TOPIC}"]
        consumer_group: ""
        start_from_oldest: true
        batching:
          period: 2s
          byte_size: 8388608
          processors:
            - protobuf_deserializer:
                protobuf_path: ./protos
                value_message: "${PROTO_VALUE}"
                key_message: "${PROTO_KEY}"
                clear_key: false

pipeline:
  threads: 0
  processors:
    - mapping: |
        root = this
        root.metadata.offset = meta("kafka_offset").number()
        root.metadata.partition = meta("kafka_partition").number()
        root.metadata.timestamp = meta("kafka_timestamp_unix").number() * 1000 * 1000
        root.metadata.is_tombstone = meta("kafka_tombstone_message").bool()

output:
  # stdout:
  #   codec: lines
  bqwrite:
    project: "${BQ_PROJECT}"
    dataset: "${BQ_DATASET}"
    table: "${BQ_TABLE}"
    protobuf_path: "./protos"
    protobuf_name: "${PROTO_VALUE}"
    max_in_flight: 5
    batching:
      period: 2s
